"""
VOSTOK-1 :: Model Trainer Pipeline
===================================
Pipeline de treinamento batch para Meta-Labeling com Random Forest.
Processa histÃ³rico de trades do Sniper e gera modelo preditivo.

Arquiteto: Petrovich | Operador: Vostok
Stack: Python 3.11 + scikit-learn + pandas + joblib
"""

import json
import logging
import os
import sys
from datetime import datetime
from pathlib import Path
from typing import Any

import joblib
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (
    precision_score,
    recall_score,
    f1_score,
    accuracy_score,
    classification_report,
    confusion_matrix,
)
from sklearn.model_selection import train_test_split

# ============================================================================
# CONFIGURAÃ‡ÃƒO DE LOGGING
# ============================================================================
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)s | %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S',
    handlers=[logging.StreamHandler(sys.stdout)]
)
logger = logging.getLogger("trainer")

# ============================================================================
# CONFIGURAÃ‡Ã•ES
# ============================================================================
DATA_DIR = Path(os.getenv("DATA_DIR", "/app/data"))
MODELS_DIR = Path(os.getenv("MODELS_DIR", "/app/models"))
TRAINING_DIR = DATA_DIR / "training"
DATASET_FILE = TRAINING_DIR / "dataset.jsonl"
MODEL_FILE = MODELS_DIR / "sniper_v1.pkl"
METRICS_FILE = MODELS_DIR / "model_metrics.json"

# Requisitos mÃ­nimos
MIN_SAMPLES = 50
MIN_PRECISION = 0.35  # Threshold de qualidade (nÃ£o aceita spam)

# Threshold de probabilidade para previsÃ£o
PROBA_THRESHOLD = 0.70  # SÃ³ considera sinal se confianÃ§a > 70% (mais seletivo)

# Features a extrair
FEATURE_NAMES = [
    "rsi",
    "cvd",
    "entropy",
    "volatility_atr",
    "funding_rate",
]


# ============================================================================
# STEP 1: INGESTÃƒO DE DADOS
# ============================================================================
def load_dataset() -> pd.DataFrame | None:
    """
    Carrega o dataset JSONL e retorna como DataFrame.
    Retorna None se dados insuficientes.
    """
    logger.info("=" * 60)
    logger.info("STEP 1: INGESTÃƒO DE DADOS")
    logger.info("=" * 60)
    
    if not DATASET_FILE.exists():
        logger.error(f"Dataset nÃ£o encontrado: {DATASET_FILE}")
        return None
    
    # Ler arquivo JSONL
    records = []
    with open(DATASET_FILE, 'r', encoding='utf-8') as f:
        for line_num, line in enumerate(f, 1):
            line = line.strip()
            if not line:
                continue
            try:
                record = json.loads(line)
                records.append(record)
            except json.JSONDecodeError as e:
                logger.warning(f"Linha {line_num} ignorada (JSON invÃ¡lido): {e}")
    
    n_samples = len(records)
    logger.info(f"ğŸ“ Amostras encontradas: {n_samples}")
    
    # VerificaÃ§Ã£o de mÃ­nimo
    if n_samples < MIN_SAMPLES:
        logger.warning(
            f"âš ï¸  WARN: Dataset incipiente ({n_samples} linhas). "
            f"Aguardando mais trades do Sniper. MÃ­nimo necessÃ¡rio: {MIN_SAMPLES}"
        )
        return None
    
    df = pd.DataFrame(records)
    logger.info(f"âœ… Dataset carregado: {len(df)} registros")
    
    return df


# ============================================================================
# STEP 2: PREPARAÃ‡ÃƒO DE FEATURES
# ============================================================================
def prepare_features(df: pd.DataFrame) -> tuple[np.ndarray, np.ndarray, list[str]] | None:
    """
    Prepara Features (X) e Target (y) para treinamento.
    """
    logger.info("")
    logger.info("=" * 60)
    logger.info("STEP 2: PREPARAÃ‡ÃƒO DE FEATURES")
    logger.info("=" * 60)
    
    # Verificar coluna target
    if 'outcome_label' not in df.columns:
        logger.error("Coluna 'outcome_label' nÃ£o encontrada no dataset")
        return None
    
    # Extrair features do objeto aninhado
    feature_data = []
    valid_indices = []
    
    for idx, row in df.iterrows():
        features = row.get('features', {})
        if not isinstance(features, dict):
            continue
        
        feature_row = {}
        valid = True
        
        for feat_name in FEATURE_NAMES:
            value = features.get(feat_name)
            if value is None:
                # Tentar nome alternativo
                if feat_name == "funding_rate":
                    value = features.get("funding", 0)
                else:
                    value = 0
            
            try:
                feat_value = float(value)
                if not np.isfinite(feat_value):
                    feat_value = 0.0
                feature_row[feat_name] = feat_value
            except (ValueError, TypeError):
                feature_row[feat_name] = 0.0
        
        if valid:
            feature_data.append(feature_row)
            valid_indices.append(idx)
    
    if not feature_data:
        logger.error("Nenhuma feature vÃ¡lida extraÃ­da")
        return None
    
    # Criar DataFrame de features
    X_df = pd.DataFrame(feature_data)
    y_series = df.loc[valid_indices, 'outcome_label']
    
    # Limpeza: remover NaN e infinitos de AMBOS X e y
    y_values = pd.to_numeric(y_series, errors='coerce')
    valid_mask = (
        ~X_df.isna().any(axis=1) & 
        ~np.isinf(X_df.values).any(axis=1) &
        y_values.notna().values
    )
    
    X_clean = X_df[valid_mask].values
    y_clean = y_values[valid_mask].values.astype(int)
    
    logger.info(f"ğŸ“Š Features extraÃ­das: {FEATURE_NAMES}")
    logger.info(f"ğŸ“Š Amostras vÃ¡lidas apÃ³s limpeza: {len(X_clean)}")
    logger.info(f"ğŸ“Š DistribuiÃ§Ã£o target:")
    logger.info(f"   - Class 0 (Loss): {sum(y_clean == 0)}")
    logger.info(f"   - Class 1 (Win):  {sum(y_clean == 1)}")
    
    if len(X_clean) < MIN_SAMPLES:
        logger.warning(f"âš ï¸  Amostras insuficientes apÃ³s limpeza: {len(X_clean)}")
        return None
    
    return X_clean, y_clean, FEATURE_NAMES


# ============================================================================
# STEP 3: TREINAMENTO
# ============================================================================
def train_model(
    X: np.ndarray, 
    y: np.ndarray, 
    feature_names: list[str]
) -> tuple[RandomForestClassifier, dict[str, Any]] | None:
    """
    Treina RandomForestClassifier com os dados preparados.
    """
    logger.info("")
    logger.info("=" * 60)
    logger.info("STEP 3: TREINAMENTO (O CÃ‰REBRO)")
    logger.info("=" * 60)
    
    # Train/Test Split (80/20, preservar ordem temporal)
    X_train, X_test, y_train, y_test = train_test_split(
        X, y,
        test_size=0.2,
        shuffle=False,  # IMPORTANTE: preservar ordem temporal
        random_state=42
    )
    
    logger.info(f"ğŸ“Š Split temporal (shuffle=False):")
    logger.info(f"   - Train: {len(X_train)} amostras")
    logger.info(f"   - Test:  {len(X_test)} amostras")
    
    # Instanciar modelo - OTIMIZADO PARA IMBALANCE
    model = RandomForestClassifier(
        n_estimators=200,              # Mais Ã¡rvores para estabilidade
        max_depth=10,                  # Limita profundidade (evitar decorar ruÃ­do)
        min_samples_leaf=50,           # Exige evidÃªncia forte
        class_weight='balanced_subsample',  # Penaliza erros na classe minoritÃ¡ria
        random_state=42,
        n_jobs=-1,
    )
    
    logger.info("ğŸ§  Treinando RandomForestClassifier (Otimizado para Imbalance)...")
    logger.info(f"   - n_estimators: 200")
    logger.info(f"   - max_depth: 10")
    logger.info(f"   - min_samples_leaf: 50")
    logger.info(f"   - class_weight: balanced_subsample")
    
    # Treinar
    model.fit(X_train, y_train)
    
    logger.info("âœ… Modelo treinado com sucesso!")
    
    # Preparar dados para validaÃ§Ã£o
    validation_data = {
        'model': model,
        'X_test': X_test,
        'y_test': y_test,
        'X_train': X_train,
        'y_train': y_train,
        'feature_names': feature_names,
    }
    
    return model, validation_data


# ============================================================================
# STEP 4: VALIDAÃ‡ÃƒO E MÃ‰TRICAS
# ============================================================================
def validate_model(model: RandomForestClassifier, validation_data: dict) -> dict[str, Any]:
    """
    Valida o modelo e calcula mÃ©tricas de performance.
    """
    logger.info("")
    logger.info("=" * 60)
    logger.info("STEP 4: VALIDAÃ‡ÃƒO E MÃ‰TRICAS")
    logger.info("=" * 60)
    
    X_test = validation_data['X_test']
    y_test = validation_data['y_test']
    feature_names = validation_data['feature_names']
    
    # PrevisÃµes usando THRESHOLD DE PROBABILIDADE (mais seletivo)
    if len(model.classes_) > 1:
        y_proba = model.predict_proba(X_test)[:, 1]
        # SÃ³ considera sinal se confianÃ§a > PROBA_THRESHOLD
        y_pred = (y_proba >= PROBA_THRESHOLD).astype(int)
        logger.info(f"ğŸšï¸  Usando threshold de probabilidade: {PROBA_THRESHOLD}")
    else:
        y_pred = model.predict(X_test)
        y_proba = None
    
    # MÃ©tricas principais
    precision = precision_score(y_test, y_pred, zero_division=0)
    recall = recall_score(y_test, y_pred, zero_division=0)
    f1 = f1_score(y_test, y_pred, zero_division=0)
    accuracy = accuracy_score(y_test, y_pred)
    
    logger.info("ğŸ“ˆ MÃ‰TRICAS DE PERFORMANCE:")
    logger.info(f"   ğŸ¯ Precision: {precision:.4f} {'âœ…' if precision > MIN_PRECISION else 'âš ï¸'}")
    logger.info(f"   ğŸ“Š Recall:    {recall:.4f}")
    logger.info(f"   ğŸ“Š F1-Score:  {f1:.4f}")
    logger.info(f"   ğŸ“Š Accuracy:  {accuracy:.4f}")
    
    # Confusion Matrix
    cm = confusion_matrix(y_test, y_pred)
    logger.info("")
    logger.info("ğŸ“Š CONFUSION MATRIX:")
    logger.info(f"   [[TN={cm[0][0]:3d}  FP={cm[0][1]:3d}]")
    logger.info(f"    [FN={cm[1][0]:3d}  TP={cm[1][1]:3d}]]")
    
    # Feature Importance
    importances = model.feature_importances_
    importance_pairs = sorted(
        zip(feature_names, importances),
        key=lambda x: x[1],
        reverse=True
    )
    
    logger.info("")
    logger.info("ğŸ” FEATURE IMPORTANCE (Top Contributors):")
    for feat_name, importance in importance_pairs:
        bar = "â–ˆ" * int(importance * 20)
        logger.info(f"   {feat_name:20s} {importance:.4f} {bar}")
    
    # Preparar mÃ©tricas para exportaÃ§Ã£o
    metrics = {
        'precision': round(precision, 4),
        'recall': round(recall, 4),
        'f1_score': round(f1, 4),
        'accuracy': round(accuracy, 4),
        'train_samples': len(validation_data['X_train']),
        'test_samples': len(X_test),
        'feature_importance': {name: round(imp, 4) for name, imp in importance_pairs},
        'confusion_matrix': cm.tolist(),
        'trained_at': datetime.now().isoformat(),
        'model_version': 'sniper_v1',
    }
    
    return metrics


# ============================================================================
# STEP 5: EXPORTAÃ‡ÃƒO
# ============================================================================
def export_model(
    model: RandomForestClassifier, 
    metrics: dict[str, Any],
    feature_names: list[str]
) -> bool:
    """
    Exporta modelo e mÃ©tricas se precision > 0.5.
    """
    logger.info("")
    logger.info("=" * 60)
    logger.info("STEP 5: EXPORTAÃ‡ÃƒO")
    logger.info("=" * 60)
    
    precision = metrics['precision']
    
    if precision < MIN_PRECISION:
        logger.warning(
            f"âš ï¸  Precision ({precision:.4f}) abaixo do mÃ­nimo ({MIN_PRECISION}). "
            f"Modelo NÃƒO serÃ¡ salvo. Mais dados necessÃ¡rios."
        )
        return False
    
    # Garantir diretÃ³rio existe
    MODELS_DIR.mkdir(parents=True, exist_ok=True)
    
    # Salvar modelo com metadados
    model_data = {
        'model': model,
        'feature_names': feature_names,
        'metrics': metrics,
        'version': 'sniper_v1',
    }
    
    joblib.dump(model_data, MODEL_FILE)
    logger.info(f"âœ… Modelo salvo: {MODEL_FILE}")
    
    # Salvar mÃ©tricas em JSON
    with open(METRICS_FILE, 'w', encoding='utf-8') as f:
        json.dump(metrics, f, indent=2, ensure_ascii=False)
    logger.info(f"âœ… MÃ©tricas salvas: {METRICS_FILE}")
    
    return True


# ============================================================================
# MAIN PIPELINE
# ============================================================================
def main() -> int:
    """Pipeline principal de treinamento."""
    logger.info("")
    logger.info("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    logger.info("â•‘   VOSTOK-1 :: MODEL TRAINER PIPELINE                        â•‘")
    logger.info("â•‘   Random Forest Meta-Labeling for Sniper Protocol           â•‘")
    logger.info("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
    logger.info("")
    
    # Step 1: IngestÃ£o
    df = load_dataset()
    if df is None:
        logger.info("")
        logger.info("Pipeline encerrado (dados insuficientes)")
        return 0  # Exit gracefully
    
    # Step 2: PreparaÃ§Ã£o
    result = prepare_features(df)
    if result is None:
        logger.error("Falha na preparaÃ§Ã£o de features")
        return 1
    
    X, y, feature_names = result
    
    # Step 3: Treinamento
    train_result = train_model(X, y, feature_names)
    if train_result is None:
        logger.error("Falha no treinamento")
        return 1
    
    model, validation_data = train_result
    
    # Step 4: ValidaÃ§Ã£o
    metrics = validate_model(model, validation_data)
    
    # Step 5: ExportaÃ§Ã£o
    success = export_model(model, metrics, feature_names)
    
    logger.info("")
    logger.info("=" * 60)
    if success:
        logger.info("ğŸ¯ PIPELINE CONCLUÃDO COM SUCESSO!")
        logger.info(f"   Modelo: {MODEL_FILE}")
        logger.info(f"   Precision: {metrics['precision']:.4f}")
    else:
        logger.info("âš ï¸  PIPELINE CONCLUÃDO (modelo nÃ£o salvo)")
    logger.info("=" * 60)
    
    return 0


if __name__ == "__main__":
    sys.exit(main())
